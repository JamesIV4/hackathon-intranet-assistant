# Summary

The purpose of this listening test is to evaluate how our current TTS voices compare to other commercially available voices. This will help develop confidence in our salesforce to sell our TTS. For this stage of testing, we will recruit people from outside of the speech group. The first voice we test will be Jill, our U.S. English voice.

# Test Sections

The listening test consists of five parts: Same-Context Intelligibility, Minimal-Pair Intelligibility, Naturalness, Continuity, and Over-all Preference. In each part, the volunteer is asked to evaluate pre-generated samples of different voices. The sections are as follows:

### 1\. Same-Context Intelligibility (e.g. Numbers)

In the same-context intelligibility test, the user is asked to identify potentially ambiguous-sounding phrases generated by our current voice (e.g.  "Which number do you hear?" with the option to select "fifteen" or "fifty").

### 2\. Minimal-Pair Intelligibility

In the minimal pairs test, the user is asked to identify potentially ambiguous-sounding words in a flexible frame (e.g. Audio plays "Say 'bad' again" with the option to select "bad" or "bat").

### 3\. Naturalness

In the naturalness test, the user is asked to rate the naturalness of a longer phrase, on a scale of 1 to 10. We will include exceptionally good (e.g. Kendra voice recordings) and exceptionally bad samples to avoid range equalization bias by establishing a larger range. We will include (fewer) samples from Polly.

### 4\. Continuity

In the continuity test, the user is asked to rate the flow of a hybrid phrase, a combination of pre-recorded voice and TTS, on a scale of 1 to 10 (e.g. "Your wait time is approximately" "twenty-two" "minutes"). We will include (fewer) samples from Polly, Google, and Watson (IBM).

### 5\. Over-all Preference

Raw preference between various pairs of voices: A/B testing.

# Test Sections

The following table shows each section of the test, the measure it uses, the voices it compares, and the number of evaluations required for that section.

**Section**| **Measure**| **Voices**| **Per Voice**| **Total Evaluations**  
---|---|---|---|---  
Intelligibility: Same Context (e.g. numbers)| Binary| DNN/Nuance| 5| 10  
Intelligibility: True Minimal Pairs| Binary| DNN/Nuance| 5| 10  
Naturalness| M.O.S.| DNN/GMM/Nuance/Polly/~~Google/Watson~~|  3| 18  
Continuity| M.O.S.| DNN/Nuance/Polly/~~Google/Watson~~|  3| 15  
Over-all Preference| A/B| DNN/Nuance, DNN/Polly, ~~DNN/Google, DNN/Watson~~|   | 12  
  
For the Continuity and Over-all Preference sections, the user interface will also offer volunteers the opportunity to add comments to their answers if they want to. (There will be a button to un-hide the input box.)

# Decisions Per Volunteer

Should each volunteer do all the evaluations or should we divide it up? (Lightens the load per volunteer but requires more volunteers and versions of the test).

# Testing Environment

The user interface for the listening test is a web browser, but the test files must be downloaded to the volunteer's computer, and headphones must be used for clearer audio.

We may be able to implement an online version so that volunteers don't have to download and extract the test files.

# Write-up

The end goal is to answer two questions:

-          Are we ready to push the DNN voice? (Our internal tests should answer this question, though)

-          How does our voice compare to the competition? (Which competition?)

We'll also include demographic data in the report (age range, gender, dialect?) but we won't try to select volunteers for demographic coverage.
